[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "I created this space to collect some projects and ideas.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "About\nI am a postdoctoral researcher in the Cognitive Engineering Laboratory (CEL) at the University of Toronto.\nMy research focuses on developing systems that augment human performance and mitigate human errors, while supporting individual and societal well-being. Specifically, I work on supervisory control of automation, human-system performance evaluation, and feedback mechanisms (e.g., transparency, explainability) that guide user behavior effectively.\nI have a background in cognitive science, computational linguistics and human-computer interaction. In my current role at CEL, I lead a team of researchers on conducting experiments to study problems in supervisory control of small modular reactors.\nPublications",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research",
    "crumbs": [
      "Home",
      "Courses"
    ]
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Davide Gentile",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.Â Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Davide Gentile",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "Human Factors in SMR Operations\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning Machine Feedback to Assist Human Decisions\n\n\n\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment of Polar Chart for Model Interpretability\n\n\n\n\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Portfolio"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nSep 12, 2024\n\n\nHow to ask causal questions\n\n\nnews\n\n\n\n\nSep 12, 2024\n\n\nQualcosa thank you\n\n\nnews\n\n\n\n\nSep 12, 2024\n\n\nConducting a sociotechnical evaluation\n\n\nmethods\n\n\n\n\nSep 12, 2024\n\n\nInvestigating collision risk factors for personalized insurance policies\n\n\nnews\n\n\n\n\nJul 18, 2023\n\n\nExploring user interaction challenges with large language models\n\n\nllms, user interaction\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "How to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\n\nApproaching AI\n\n\n\nThereâ€™s more to AI than just creating models.\n\n\n\n\n\nMay 27, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in todayâ€™s landscape is very different. AI methods are created to solve small, atomic problems. And weâ€™ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; itâ€™s only a small part of it. Itâ€™s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you donâ€™t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard â€” whoâ€™s widely known for his fastai course and library â€”, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data â€” data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, Iâ€™ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI courseâ€™s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLetâ€™s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the languageâ€™s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts â€” using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system â€” this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuÅŸ dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principleâ†©ï¸Ž"
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in todayâ€™s landscape is very different. AI methods are created to solve small, atomic problems. And weâ€™ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; itâ€™s only a small part of it. Itâ€™s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you donâ€™t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard â€” whoâ€™s widely known for his fastai course and library â€”, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data â€” data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, Iâ€™ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI courseâ€™s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLetâ€™s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the languageâ€™s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts â€” using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system â€” this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuÅŸ dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principleâ†©ï¸Ž"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "How to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\n\nApproaching AI\n\n\n\nThereâ€™s more to AI than just creating models.\n\n\n\n\n\nMay 27, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in todayâ€™s landscape is very different. AI methods are created to solve small, atomic problems. And weâ€™ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; itâ€™s only a small part of it. Itâ€™s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you donâ€™t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard â€” whoâ€™s widely known for his fastai course and library â€”, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data â€” data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, Iâ€™ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI courseâ€™s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLetâ€™s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the languageâ€™s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts â€” using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system â€” this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuÅŸ dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principleâ†©ï¸Ž"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome/index.html",
    "href": "blogposts/welcome/index.html",
    "title": "Halden HTOâ€™s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/post-with-code/index.html",
    "href": "blogposts/post-with-code/index.html",
    "title": "Choosing the design for your experiment",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "works/work1.html",
    "href": "works/work1.html",
    "title": "Project 1",
    "section": "",
    "text": "Project 1\nHere is a detailed description of Project 1, including the technologies used, challenges, and outcomes.\n\n\n\nProject Image"
  },
  {
    "objectID": "workposts/welcome/index.html",
    "href": "workposts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "workposts/post-with-code/index.html",
    "href": "workposts/post-with-code/index.html",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "",
    "text": "This is the project of my dissertation\nðŸ“„ Report\nðŸ“Š Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "work.html#predictive-modeling-for-sales-forecasting",
    "href": "work.html#predictive-modeling-for-sales-forecasting",
    "title": "works",
    "section": "Predictive Modeling for Sales Forecasting",
    "text": "Predictive Modeling for Sales Forecasting\n\nSkills: R, Python, Machine Learning, Time Series Analysis\nCategories: Data Science, Applied Statistics\nDescription: Built a predictive model to forecast sales using time series analysis and machine learning. Improved forecast accuracy by 20%."
  },
  {
    "objectID": "work.html#ai-decision-aid-evaluation",
    "href": "work.html#ai-decision-aid-evaluation",
    "title": "works",
    "section": "AI Decision Aid Evaluation",
    "text": "AI Decision Aid Evaluation\n\nSkills: Data Analysis, Statistical Modeling, Human Factors Engineering\nCategories: AI Evaluation, Decision Support Systems\nDescription: Evaluated explanation interfaces for decision-making in automated systems, focusing on false alarms and overreliance."
  },
  {
    "objectID": "work.html#exploratory-data-analysis-on-toronto-island-data",
    "href": "work.html#exploratory-data-analysis-on-toronto-island-data",
    "title": "works",
    "section": "Exploratory Data Analysis on Toronto Island Data",
    "text": "Exploratory Data Analysis on Toronto Island Data\n\nSkills: R, Data Wrangling, Visualization\nCategories: Data Science, Statistics\nDescription: Merged ticket sales data with weather datasets to understand patterns in tourism. Visualized trends to inform public policy."
  },
  {
    "objectID": "workposts/post-with-code/index.html#wilcoxon-rank-sum",
    "href": "workposts/post-with-code/index.html#wilcoxon-rank-sum",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "The impact of post-hoc explanations from automated decision aids on human performance\n\n\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nHuman performance in operation of small modular reactors\n\n\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "teaching",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Work\nPostdoctoral researcher - University of Toronto\n09/2024 â€“ present\n\nI lead the experimental and analytical teams at the Cognitive Engineering Lab on projects funded by NSERC* and CNSC** focused on remote monitoring and control of small modular reactors.\nBetween 2019 and 2024 I was PhD researcher and teaching assistant in statistical programming and human factors engineering.\nMy PhD focused on designing and evaluating explainable AI displays to support human decisions and performance in safety-critical work environments.\n\nData science consultant - Armilla AI\n01/2024 - 06/2024\nI worked part-time in this Toronto-based startup, where I have developed risk assessment frameworks to evaluate predictive and generative models across different dimensions, including system performance and model bias.\nResearch intern - Ericsson\n10/2020 - 06/2024\nI led the human-subjects evaluation of a machine learning software designed by my team to support the productivity and performance of Ericsson's data scientists.\nExpertise\nStatistics\nDesign of experiments\nMachine learning\nHuman-subjects research\nCausal inference\nPrototyping\nCross-functional collaboration\nTools\nR, RStudio\nPython\nQuarto\nBooks, papers\nLanguages\nItalian\nEnglish\nLatin\nBasic Spanish and French"
  },
  {
    "objectID": "docs/courses.html",
    "href": "docs/courses.html",
    "title": "courses",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research"
  },
  {
    "objectID": "research - Copy.html",
    "href": "research - Copy.html",
    "title": "research",
    "section": "",
    "text": "AI Decision Aid Evaluation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sites.html",
    "href": "sites.html",
    "title": "sites",
    "section": "",
    "text": "AI Decision Aid Evaluation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTristan Oâ€™Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workposts/time-series/index.html",
    "href": "workposts/time-series/index.html",
    "title": "Time series analysis for customer retention",
    "section": "",
    "text": "This is a time series analysis\nðŸ“„ Report\nðŸ“Š Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "workposts/time-series/index.html#wilcoxon-rank-sum",
    "href": "workposts/time-series/index.html#wilcoxon-rank-sum",
    "title": "Time series analysis for customer retention",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/small-modular-reactors/index.html",
    "href": "researchposts/small-modular-reactors/index.html",
    "title": "Human performance in operation of small modular reactors",
    "section": "",
    "text": "This is the project of my dissertation\nðŸ“„ Report\nðŸ“Š Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/small-modular-reactors/index.html#wilcoxon-rank-sum",
    "href": "researchposts/small-modular-reactors/index.html#wilcoxon-rank-sum",
    "title": "Human performance in operation of small modular reactors",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/explanations/index.html",
    "href": "researchposts/explanations/index.html",
    "title": "Explaining automated results in decision support systems",
    "section": "",
    "text": "This is a time series analysis\nðŸ“„ Report\nðŸ“Š Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/explanations/index.html#wilcoxon-rank-sum",
    "href": "researchposts/explanations/index.html#wilcoxon-rank-sum",
    "title": "Explaining automated results in decision support systems",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/human-performance/index.html",
    "href": "researchposts/human-performance/index.html",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/Conducting a socio-technical evaluation/index.html",
    "href": "blogposts/Conducting a socio-technical evaluation/index.html",
    "title": "Conducting a sociotechnical evaluation",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome - Copy/index.html",
    "href": "blogposts/welcome - Copy/index.html",
    "title": "Halden HTOâ€™s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome - Copy (2)/index.html",
    "href": "blogposts/welcome - Copy (2)/index.html",
    "title": "Halden HTOâ€™s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/asking-causal-questions/index.html",
    "href": "blogposts/asking-causal-questions/index.html",
    "title": "How to ask causal questions",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "researchposts/explanations - Copy/index.html",
    "href": "researchposts/explanations - Copy/index.html",
    "title": "Usability study of XAI displays to support productivity of Ericssonâ€™s data scientists",
    "section": "",
    "text": "This is a time series analysis\nðŸ“„ Report\nðŸ“Š Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/explanations - Copy/index.html#wilcoxon-rank-sum",
    "href": "researchposts/explanations - Copy/index.html#wilcoxon-rank-sum",
    "title": "Usability study of XAI displays to support productivity of Ericssonâ€™s data scientists",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "blogposts/choosing-design-experiment/index.html",
    "href": "blogposts/choosing-design-experiment/index.html",
    "title": "Exploring user interaction challenges with large language models",
    "section": "",
    "text": "https://srinstitute.utoronto.ca/news/exploring-user-interaction-challenges-with-large-language-models"
  },
  {
    "objectID": "blogposts/driving-insurance/index.html",
    "href": "blogposts/driving-insurance/index.html",
    "title": "Investigating collision risk factors for personalized insurance policies",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "workposts/explanations-human-performance/index.html",
    "href": "workposts/explanations-human-performance/index.html",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "",
    "text": "This is the project of my dissertation\nimage: https://www.geeksforgeeks.org/explainable-artificial-intelligencexai/\nðŸ“„ Report\nðŸ“Š Slides\nlibrary(ggrain)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nsdt &lt;- read_csv(\"sdt.csv\")\nsdt$cond = as.factor(sdt$cond)\nsdt$measure = as.factor(sdt$measure)\n\nsensitivity  &lt;-  sdt %&gt;% \n  filter(measure == \"dprime\")\nsensitivity$measure = as.factor(sensitivity$measure)\n\n\ndprimesum &lt;- summarySE(sensitivity, measurevar = \"score\", groupvars=c(\"cond\", \"measure\"))\n\nhead(dprimesum)\n\n  cond measure  N score_mean score_median        sd       sem        ci\n1    0  dprime 25  0.7685624    0.7823470 0.9889596 0.1977919 0.4082225\n2    1  dprime 25  0.9793959    0.6523937 0.8077070 0.1615414 0.3334051\n3    2  dprime 25  1.8532857    2.1213820 0.9431285 0.1886257 0.3893043\n\nggplot(sensitivity, aes(cond, score, fill = measure)) +\n  geom_rain(alpha = .5) +\n  theme_classic() +\n  scale_fill_brewer(palette = 'Dark2') +\n  guides(fill = 'none', color = 'none')"
  },
  {
    "objectID": "workposts/explanations-human-performance/index.html#wilcoxon-rank-sum",
    "href": "workposts/explanations-human-performance/index.html#wilcoxon-rank-sum",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/driving-data/index.html",
    "href": "workposts/driving-data/index.html",
    "title": "sk-analysis-2024",
    "section": "",
    "text": "In this document I keep track of the analysis plan for the SK Networks dataset. The objective is to write a good paper for publication. The idea of the analysis is the following:\n\nUse ML to identify the factors related to collision involvement (0,1) and to collision severity (3 levels, or cumulative damage cost)\nConduct causal inference analysis to identify the impact of the factors associated with collision involvement\nConduct multinomial or ordered logistic regression to identify the impact of the factors associated with collision severity; this should have adjustments that enable to do causal analysis, but I donâ€™t know which adjustments yet."
  },
  {
    "objectID": "workposts/driving-data/index.html#wilcoxon-rank-sum",
    "href": "workposts/driving-data/index.html#wilcoxon-rank-sum",
    "title": "Driving analytics for risk assessment in usage-based insurance",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/ericsson-project/index.html",
    "href": "workposts/ericsson-project/index.html",
    "title": "Development of Polar Chart for Model Interpretability",
    "section": "",
    "text": "Here I report the development of a glyph-based polar chart (GPC), designed to support a comprehensive interpretation of the results of ML models by enabling data scientists to compare different explanation methods within the same model and across models.\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Causal inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData strategy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel audits\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "serviceposts/audit/index.html",
    "href": "serviceposts/audit/index.html",
    "title": "Model audit",
    "section": "",
    "text": "This is a time series analysis\nðŸ“„ Report\nðŸ“Š Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "serviceposts/audit/index.html#wilcoxon-rank-sum",
    "href": "serviceposts/audit/index.html#wilcoxon-rank-sum",
    "title": "Model audit",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/data/index.html",
    "href": "servicesposts/data/index.html",
    "title": "Data strategy",
    "section": "",
    "text": "Develop a data strategy that aligns with your business goals, focusing on data collection, analysis, and experimentation.\nðŸ“„ Report\nðŸ“Š Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "servicesposts/data/index.html#wilcoxon-rank-sum",
    "href": "servicesposts/data/index.html#wilcoxon-rank-sum",
    "title": "Data strategy",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/audit/index.html",
    "href": "servicesposts/audit/index.html",
    "title": "Model audits",
    "section": "",
    "text": "Assess the performance of your models (predictive, generative), identify biases and ensure robustness to support accurate and reliable outcomes.\nðŸ“„ Report\nðŸ“Š Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "servicesposts/audit/index.html#wilcoxon-rank-sum",
    "href": "servicesposts/audit/index.html#wilcoxon-rank-sum",
    "title": "Model audits",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/inference/index.html",
    "href": "servicesposts/inference/index.html",
    "title": "Causal inference",
    "section": "",
    "text": "Uncover the cause-and-effect relationships in your data using methodologies like randomized controlled trials and regression discontinuity.\n\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#menu-sidebar",
    "href": "index.html#menu-sidebar",
    "title": "Frameset Example Title",
    "section": "Menu (Sidebar)",
    "text": "Menu (Sidebar)\nYou can create a sidebar with R Markdown using htmltools or simply present the menu in a separate section. If you prefer to embed the menu HTML directly:"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Frameset Example Title",
    "section": "Content",
    "text": "Content\nThis is the main content area, which was originally the â€œcontentâ€ frame. You can embed the HTML content like so:"
  },
  {
    "objectID": "workposts/hf-nuclear/index.html",
    "href": "workposts/hf-nuclear/index.html",
    "title": "Human Factors in SMR Operations",
    "section": "",
    "text": "Small Modular Reactors (SMRs) are modern types of reactors that presents novel characteristics compared to traditional reactors. These characteristics allow for remote monitoring and control of multiple facilities, which introduces a new set of human factors challenges related to the impact of control room design on human performance.\n\n\n\nThe basic principle iPWR simulator setup in the Halden Future Lab"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Peer reviewed journal articles\nGentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\nPeer reviewed conference/workshop proceedings\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\n\nArticles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI â€™21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/publications/index.html",
    "href": "workposts/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "List of academic publications\n\nPeer reviewed journal articles\n\nGoogle Scholar\nGentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\n\nPeer reviewed conference/workshop proceedings\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\n\nArticles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI â€™21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/1-hf-nuclear/index.html",
    "href": "workposts/1-hf-nuclear/index.html",
    "title": "Human Factors in SMR Operations",
    "section": "",
    "text": "Small Modular Reactors (SMRs) are modern types of reactors that presents novel characteristics compared to traditional reactors. These characteristics allow for remote monitoring and control of multiple facilities, which introduces a new set of human factors challenges related to the impact of control room design on human performance.\n\n\n\nThe basic principle iPWR simulator setup in the Halden Future Lab"
  },
  {
    "objectID": "workposts/4-publications/index.html",
    "href": "workposts/4-publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "List of academic publications\n\nPeer reviewed journal articles\n\nGoogle Scholar\nGentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\n\nPeer reviewed conference/workshop proceedings\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\n\nArticles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI â€™21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/2-explanations-human-performance/index.html",
    "href": "workposts/2-explanations-human-performance/index.html",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "",
    "text": "This is the project of my dissertation\nimage: https://www.geeksforgeeks.org/explainable-artificial-intelligencexai/\nðŸ“„ Report\nðŸ“Š Slides\nlibrary(ggrain)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nsdt &lt;- read_csv(\"sdt.csv\")\nsdt$cond = as.factor(sdt$cond)\nsdt$measure = as.factor(sdt$measure)\n\nsensitivity  &lt;-  sdt %&gt;% \n  filter(measure == \"dprime\")\nsensitivity$measure = as.factor(sensitivity$measure)\n\n\ndprimesum &lt;- summarySE(sensitivity, measurevar = \"score\", groupvars=c(\"cond\", \"measure\"))\n\nhead(dprimesum)\n\n  cond measure  N score_mean score_median        sd       sem        ci\n1    0  dprime 25  0.7685624    0.7823470 0.9889596 0.1977919 0.4082225\n2    1  dprime 25  0.9793959    0.6523937 0.8077070 0.1615414 0.3334051\n3    2  dprime 25  1.8532857    2.1213820 0.9431285 0.1886257 0.3893043\n\nggplot(sensitivity, aes(cond, score, fill = measure)) +\n  geom_rain(alpha = .5) +\n  theme_classic() +\n  scale_fill_brewer(palette = 'Dark2') +\n  guides(fill = 'none', color = 'none')"
  },
  {
    "objectID": "workposts/2-explanations-human-performance/index.html#wilcoxon-rank-sum",
    "href": "workposts/2-explanations-human-performance/index.html#wilcoxon-rank-sum",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructorsâ€™ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where â€˜studentâ€™ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis â€˜studentâ€™ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemarâ€™s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/3-ericsson-project/index.html",
    "href": "workposts/3-ericsson-project/index.html",
    "title": "Development of Polar Chart for Model Interpretability",
    "section": "",
    "text": "Here I report the development of a glyph-based polar chart (GPC), designed to support a comprehensive interpretation of the results of ML models by enabling data scientists to compare different explanation methods within the same model and across models.\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]