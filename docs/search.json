[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Human Factors & AI Consulting",
    "section": "",
    "text": "Many safety-critical industries introduce automation without considering human cognitive responses. This can lead to operator overload, poor decision-making, and compliance risks. I help organizations create AI systems that supports human performance while meeting NRC, IAEA, and CNSC standards.\n\nSolutions:\n✅ AI Usability and Cognitive Load Assessments\n✅ Trust and Explainability Audits\n✅ Compliance Guidance for AI Systems"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "Automation should empower human operators, not replace them. I specialize in human factors and AI usability to ensure that AI-driven systems in safety-critical industries enhance performance, trust, and compliance.\nI have a background in human-AI interaction, cognitive engineering, and regulatory compliance. I help organizations design automation that works for people. My expertise spans machine learning explainability, decision-support design, and usability assessments in high-stakes domains like nuclear energy, aerospace, healthcare, and defense.\nI offer consulting, assessments, and training to help teams:\n✔ Improve human-AI collaboration.\n✔ Align AI explainability with decision-making needs.\n✔ Meet safety and regulatory standards.\n📩 Contact me to discuss how we can optimize your AI for usability, trust, and safety."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Case Studies in Human Factors and Ergonomics (MIE345)\nOverview: A comprehensive analysis of case studies where human factors methods have been applied to enhance human-machine system performance. The examples span both fundamental ergonomics and high-tech applications. Emphasis placed on the practical application of concepts and techniques covered in previous human factors courses.\nYears: 2024\nHuman-Centered Systems Design (MIE240)\nOverview: Introduction to principles, methods, and tools for the analysis, design, and evaluation of human-centred systems. Consideration of impacts of human physical, physiological, perceptual, and cognitive factors on the design and use of engineered systems. Basic concepts of anthropometrics, work-related hazards, shiftwork, workload, human error and reliability, system complexity, and human factors standards. The human-centered systems design process, including task analysis, user requirements generation, prototyping, and usability evaluation. Design of work/rest schedules, procedures, displays and controls, and information and training systems; design for error prevention and human-computer interaction; design for accessibility and aging populations.\nYears: 2022, 2023, 2024\nStatistics (MIE237)\nOverview: Introduction to statistics using R. Topics included: Data gathering motivation and methods (observational vs. experimental). Modeling for inference vs. prediction. Data visualizations. Two sample estimation and hypothesis testing. Choice of sample size. Fitting distributions to data. Goodness of fit tests. Simple linear regression and correlation. Multiple linear regression. Model building and model assessment. Design and analysis of single and multi-factor experiments. Analysis of variance. Fixed and random effects models. Multiple comparisons.\nYears: 2021, 2022, 2023"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Davide Gentile",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Davide Gentile",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "work.html",
    "href": "work.html",
    "title": "Portfolio",
    "section": "",
    "text": "Human performance in operation of automated nuclear reactors\n\n\n\n\n\nThis research program addresses emerging human factors challenges in the operation of small modular reactors (SMRs). The program focuses on two main objectives: i) improving methodologies for measuring situation awareness and workload in nuclear control rooms, and ii) developing a taxonomy to categorize various SMR designs and their implications for human performance. The first experiment evaluates multiple situation awareness and workload metrics across different levels of automation in computerized nuclear procedures. A parallel project within the program is creating a taxonomy that will guide the development of human factors guidelines across a range of SMR designs. These outcomes will help establish more effective performance metrics and inform the creation of standardized guidelines for SMR operations, contributing to improved safety and efficiency in the nuclear sector. This research is funded by the Natural Sciences and Engineering Council of Canada and the Canadian Nuclear Safety Commission.\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHuman-AI collaboration in industrial process control\n\n\n\n\n\nThis project investigated the impact of model-agnostic explanations on human performance in industrial process control, focusing on improving the detection of system failures in condition-based maintenance. Two controlled experiments tested the effects of different explanation types—normative, contrastive, and counterfactual—on decision-making, workload, and reliance on automated decision aids. Results showed that combining normative and contrastive explanations improved decision time and reduced workload, while adding counterfactuals further enhanced accuracy and reduced false alarms. The findings can inform the design of more effective and efficient explainable AI systems to support human operators in safety-critical work environments.\n\n\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation of machine learning models for usage-based insurance\n\n\n\n\n\nThis project leveraged fleet telematics data to inform usage-based insurance models for corporate vehicle fleets. We compared machine learning algorithms to predict collision risk in real-time driving behavior data from 3,854 corporate vehicle drivers. The analysis identified key factors that influenced collision involvement, such as driving time, trip frequency, and rapid speed changes. Fleet rental companies can use these insights to adjust insurance rates based on risky driver behaviors. This project contributes to the development of more accurate risk models and improved operational strategies for commercial fleet services.\n\n\n\n\n\nAug 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUX analysis of explainable machine learning tools for Ericsson’s data scientists\n\n\n\n\n\nThis project developed a glyph-based polar chart (GPC) to enhance the interpretability of machine learning models for data scientists. The tool enables comparisons of explanations across different models and computational methods. User experience evaluations with Ericsson data scientists showed that the GPC helped identify key model variables, compare various explanation techniques, and perform logical reviews of model outputs. This project was conducted during my internship at Ericsson’s Global AI Accelerator. \n\n\n\n\n\nJun 12, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "This section of the website is currently under construction. As of January 23rd, the only available article is the one on user interaction with llms.\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nSep 12, 2024\n\n\nAsking better causal questions\n\n\n\n\nSep 12, 2024\n\n\nEvaluating joint human-automation performance\n\n\n\n\nSep 12, 2024\n\n\nInvestigating collision risk factors for personalized insurance policies\n\n\n\n\nJul 18, 2023\n\n\nExploring user interaction challenges with large language models\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "How to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\n\nApproaching AI\n\n\n\nThere’s more to AI than just creating models.\n\n\n\n\n\nMay 27, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "docs/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "docs/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principle↩︎"
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "docs/blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principle↩︎"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "How to Approach Creating AI Models\n\n\nPutting the Drive into the Train\n\n\n\nApproaching AI\n\n\n\nThere’s more to AI than just creating models.\n\n\n\n\n\nMay 27, 2022\n\n\nSalman Naqvi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html",
    "title": "How to Approach Creating AI Models",
    "section": "",
    "text": "This article was rewritten on Monday, 31 October 2022."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#introduction",
    "title": "How to Approach Creating AI Models",
    "section": "Introduction",
    "text": "Introduction\nHow you approach making models is crucial. The way AI methods are used in today’s landscape is very different. AI methods are created to solve small, atomic problems. And we’ve got most of the methods to handle these small tasks hammered down. Therefore, applied AI is not about creating models; it’s only a small part of it. It’s 80% problem solving and 20% implementing (I would not be surprised if it actually followed the 80-20 rule1).\nThink of AI methods as a tool; think of it as a pencil. You can use pencils to draw, take notes, poke holes, and much more. There are also dozens of pencils out there. But what point is there in using any of those pencils if you don’t even know how to properly use a pencil in the first place? The art of creating pencils has already been perfected too.\nOne highly successful approach is the Drivetrain Approach, created by Jeremy Howard — who’s widely known for his fastai course and library —, Margit Zwemer, and Mike Loukides.\nThe goal of the Drivetrain Approach is to not just use data to generate more data — data that is in the form of predictions. But rather to use data to also generate actionable outcomes.\nThe official blogpost goes into much more depth here.\nIn this post, I’ll be providing a short overview of my understanding of this approach by applying it to the Elements of AI course’s final project (this online course was created by the University of Helsinki and Reaktor)."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#overview-of-the-drivetrain-approach",
    "title": "How to Approach Creating AI Models",
    "section": "Overview of the Drivetrain Approach",
    "text": "Overview of the Drivetrain Approach\nThere are four main steps to this approach:\n\nDefine the objective\nConsider your possible actions\nConsider your data\nCreate the models\n\n\n\n\nImage Source\n\n\n\nDefine the objective\nWrite out what you are really trying to achieve. What is your goal? Writing it out puts it in a tangible manner.\n\n\nConsider your actions\nThink about what actions you can take to achieve your objective.\nAlso think about what would happen if you did those actions.\nWhat would happen if I did x? Would y really be a good idea? What if z worked out too well? Will x lead to y? What would happen if x turned out poorly?\n\n\nConsider your data\nThink about the data you already have and how it could be used.\nThink about any further data that is needed and how it could be collected.\n\n\nCreate the models\nCreate models. But create models that produce actions. Actions that produce the best results for your objective."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#endangered-language-chatbot",
    "title": "How to Approach Creating AI Models",
    "section": "Endangered Language Chatbot",
    "text": "Endangered Language Chatbot\nThe final project of the Elements of AI course asked me to come up with my own AI method that would solve a problem, and how it would do so.\nThe problem I tackled was the endangerment of languages. The solution I came up with was to create a chatbot that could converse in these endangered languages. I created an overview of how this could be done.\nThe overview can be read here.\nLet’s tackle this problem through the Drivetrain Approach.\n\nDefine the objective\nThe objective is to preserve languages that are in danger of going extinct. Through preserving languages, histories and cultures can be preserved.\n\n\nConsider your actions\nOne way this could be done is to create a chatbot that could converse in endangered languages. However, this would be a monumental task considering the amount of data needed to achieve this.\nAnother action that could be taken is to create an information retrieval (IR) system of sorts. A corpus of written text of the language could be provided, from which insights about the language’s history, culture, and way of conversing could be gained. In turn the language is preserved.\nThe latter action may be easier to achieve.\n\n\nConsider your data\nThe obvious source of data would be a corpora of text.\nHowever, a major problem arises for those languages which are only spoken. Audio recordings of conversations would have to be made which would take a lot of time and effort. This would be especially difficult for those languages where very few speakers remain.\nEven if a language does have written text, gathering enough text for the language can also be a problem: the language may not have much written text. This may especially be the case for endangered languages. Again, one solution is to manually create texts — using an NLP method to create these texts is not viable.\nIn short, for some languages, there may be no choice other than to manually create the data that would be fed into the system — this manual creation also has the chance to skew the performance of the model.\n\n\n\nKuş dili, a whistled language spoken in Turkey. How would such a language be preserved? Image Source\n\n\n\n\nCreate the model\nEither a chatbot needs to be created that speaks as accurately as a native speaker, or an IR system needs to be created that gives meaningful, correct insights into a language and its associated culture.\nThis step may either be easy or hard, depending on the language. Most NLP or IR systems have been built on a few, select languages. Perhaps this step may be easy for those languages that are similar to languages on which NLP or IR systems have already been built on. It will most likely be harder for those languages which are not."
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#conclusion",
    "title": "How to Approach Creating AI Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis concludes my understanding of the Drivetrain Approach, through an example.\nApproaches are crucial: you can have state-of-the-art tools, but they are useless if not correctly applied. The approach you take can either make it or break it. Putting it into a concrete, organized, tangible manner goes a long way.\nIf you have any comments, questions, suggestions, feedback, criticisms, or corrections, please do post them down in the comment section below!"
  },
  {
    "objectID": "blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "href": "blog/posts/1_how_to_approach_creating_ai_models.html#footnotes",
    "title": "How to Approach Creating AI Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 80/20 Rule, also known as the Pareto Principle↩︎"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome/index.html",
    "href": "blogposts/welcome/index.html",
    "title": "Halden HTO’s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/post-with-code/index.html",
    "href": "blogposts/post-with-code/index.html",
    "title": "Choosing the design for your experiment",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "works/work1.html",
    "href": "works/work1.html",
    "title": "Project 1",
    "section": "",
    "text": "Project 1\nHere is a detailed description of Project 1, including the technologies used, challenges, and outcomes.\n\n\n\nProject Image"
  },
  {
    "objectID": "workposts/welcome/index.html",
    "href": "workposts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "workposts/post-with-code/index.html",
    "href": "workposts/post-with-code/index.html",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "",
    "text": "This is the project of my dissertation\n📄 Report\n📊 Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "work.html#predictive-modeling-for-sales-forecasting",
    "href": "work.html#predictive-modeling-for-sales-forecasting",
    "title": "works",
    "section": "Predictive Modeling for Sales Forecasting",
    "text": "Predictive Modeling for Sales Forecasting\n\nSkills: R, Python, Machine Learning, Time Series Analysis\nCategories: Data Science, Applied Statistics\nDescription: Built a predictive model to forecast sales using time series analysis and machine learning. Improved forecast accuracy by 20%."
  },
  {
    "objectID": "work.html#ai-decision-aid-evaluation",
    "href": "work.html#ai-decision-aid-evaluation",
    "title": "works",
    "section": "AI Decision Aid Evaluation",
    "text": "AI Decision Aid Evaluation\n\nSkills: Data Analysis, Statistical Modeling, Human Factors Engineering\nCategories: AI Evaluation, Decision Support Systems\nDescription: Evaluated explanation interfaces for decision-making in automated systems, focusing on false alarms and overreliance."
  },
  {
    "objectID": "work.html#exploratory-data-analysis-on-toronto-island-data",
    "href": "work.html#exploratory-data-analysis-on-toronto-island-data",
    "title": "works",
    "section": "Exploratory Data Analysis on Toronto Island Data",
    "text": "Exploratory Data Analysis on Toronto Island Data\n\nSkills: R, Data Wrangling, Visualization\nCategories: Data Science, Statistics\nDescription: Merged ticket sales data with weather datasets to understand patterns in tourism. Visualized trends to inform public policy."
  },
  {
    "objectID": "workposts/post-with-code/index.html#wilcoxon-rank-sum",
    "href": "workposts/post-with-code/index.html#wilcoxon-rank-sum",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "The impact of post-hoc explanations from automated decision aids on human performance\n\n\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 18, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nHuman performance in operation of small modular reactors\n\n\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "teaching",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Work\nPostdoctoral researcher - University of Toronto\n09/2024 – present\n\nI lead the experimental and analytical teams at the Cognitive Engineering Lab on projects funded by NSERC* and CNSC** focused on remote monitoring and control of small modular reactors.\nBetween 2019 and 2024 I was PhD researcher and teaching assistant in statistical programming and human factors engineering.\nMy PhD focused on designing and evaluating explainable AI displays to support human decisions and performance in safety-critical work environments.\n\nData science consultant - Armilla AI\n01/2024 - 06/2024\nI worked part-time in this Toronto-based startup, where I have developed risk assessment frameworks to evaluate predictive and generative models across different dimensions, including system performance and model bias.\nResearch intern - Ericsson\n10/2020 - 06/2024\nI led the human-subjects evaluation of a machine learning software designed by my team to support the productivity and performance of Ericsson's data scientists.\nExpertise\nStatistics\nDesign of experiments\nMachine learning\nHuman-subjects research\nCausal inference\nPrototyping\nCross-functional collaboration\nTools\nR, RStudio\nPython\nQuarto\nBooks, papers\nLanguages\nItalian\nEnglish\nLatin\nBasic Spanish and French"
  },
  {
    "objectID": "docs/courses.html",
    "href": "docs/courses.html",
    "title": "courses",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research"
  },
  {
    "objectID": "research - Copy.html",
    "href": "research - Copy.html",
    "title": "research",
    "section": "",
    "text": "AI Decision Aid Evaluation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "sites.html",
    "href": "sites.html",
    "title": "sites",
    "section": "",
    "text": "AI Decision Aid Evaluation\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nDavide Gentile\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "workposts/time-series/index.html",
    "href": "workposts/time-series/index.html",
    "title": "Time series analysis for customer retention",
    "section": "",
    "text": "This is a time series analysis\n📄 Report\n📊 Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "workposts/time-series/index.html#wilcoxon-rank-sum",
    "href": "workposts/time-series/index.html#wilcoxon-rank-sum",
    "title": "Time series analysis for customer retention",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/small-modular-reactors/index.html",
    "href": "researchposts/small-modular-reactors/index.html",
    "title": "Human performance in operation of small modular reactors",
    "section": "",
    "text": "This is the project of my dissertation\n📄 Report\n📊 Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/small-modular-reactors/index.html#wilcoxon-rank-sum",
    "href": "researchposts/small-modular-reactors/index.html#wilcoxon-rank-sum",
    "title": "Human performance in operation of small modular reactors",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/explanations/index.html",
    "href": "researchposts/explanations/index.html",
    "title": "Explaining automated results in decision support systems",
    "section": "",
    "text": "This is a time series analysis\n📄 Report\n📊 Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/explanations/index.html#wilcoxon-rank-sum",
    "href": "researchposts/explanations/index.html#wilcoxon-rank-sum",
    "title": "Explaining automated results in decision support systems",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "researchposts/human-performance/index.html",
    "href": "researchposts/human-performance/index.html",
    "title": "Evaluating the impact of automated decision aids on human performance",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/Conducting a socio-technical evaluation/index.html",
    "href": "blogposts/Conducting a socio-technical evaluation/index.html",
    "title": "Conducting a sociotechnical evaluation",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome - Copy/index.html",
    "href": "blogposts/welcome - Copy/index.html",
    "title": "Halden HTO’s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/welcome - Copy (2)/index.html",
    "href": "blogposts/welcome - Copy (2)/index.html",
    "title": "Halden HTO’s AI in Nuclear",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blogposts/asking-causal-questions/index.html",
    "href": "blogposts/asking-causal-questions/index.html",
    "title": "Asking better causal questions",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "researchposts/explanations - Copy/index.html",
    "href": "researchposts/explanations - Copy/index.html",
    "title": "Usability study of XAI displays to support productivity of Ericsson’s data scientists",
    "section": "",
    "text": "This is a time series analysis\n📄 Report\n📊 Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "researchposts/explanations - Copy/index.html#wilcoxon-rank-sum",
    "href": "researchposts/explanations - Copy/index.html#wilcoxon-rank-sum",
    "title": "Usability study of XAI displays to support productivity of Ericsson’s data scientists",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "blogposts/choosing-design-experiment/index.html",
    "href": "blogposts/choosing-design-experiment/index.html",
    "title": "Exploring user interaction challenges with large language models",
    "section": "",
    "text": "Available here."
  },
  {
    "objectID": "blogposts/driving-insurance/index.html",
    "href": "blogposts/driving-insurance/index.html",
    "title": "Investigating collision risk factors for personalized insurance policies",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "workposts/explanations-human-performance/index.html",
    "href": "workposts/explanations-human-performance/index.html",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "",
    "text": "This is the project of my dissertation\nimage: https://www.geeksforgeeks.org/explainable-artificial-intelligencexai/\n📄 Report\n📊 Slides\nlibrary(ggrain)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nsdt &lt;- read_csv(\"sdt.csv\")\nsdt$cond = as.factor(sdt$cond)\nsdt$measure = as.factor(sdt$measure)\n\nsensitivity  &lt;-  sdt %&gt;% \n  filter(measure == \"dprime\")\nsensitivity$measure = as.factor(sensitivity$measure)\n\n\ndprimesum &lt;- summarySE(sensitivity, measurevar = \"score\", groupvars=c(\"cond\", \"measure\"))\n\nhead(dprimesum)\n\n  cond measure  N score_mean score_median        sd       sem        ci\n1    0  dprime 25  0.7685624    0.7823470 0.9889596 0.1977919 0.4082225\n2    1  dprime 25  0.9793959    0.6523937 0.8077070 0.1615414 0.3334051\n3    2  dprime 25  1.8532857    2.1213820 0.9431285 0.1886257 0.3893043\n\nggplot(sensitivity, aes(cond, score, fill = measure)) +\n  geom_rain(alpha = .5) +\n  theme_classic() +\n  scale_fill_brewer(palette = 'Dark2') +\n  guides(fill = 'none', color = 'none')"
  },
  {
    "objectID": "workposts/explanations-human-performance/index.html#wilcoxon-rank-sum",
    "href": "workposts/explanations-human-performance/index.html#wilcoxon-rank-sum",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/driving-data/index.html",
    "href": "workposts/driving-data/index.html",
    "title": "sk-analysis-2024",
    "section": "",
    "text": "In this document I keep track of the analysis plan for the SK Networks dataset. The objective is to write a good paper for publication. The idea of the analysis is the following:\n\nUse ML to identify the factors related to collision involvement (0,1) and to collision severity (3 levels, or cumulative damage cost)\nConduct causal inference analysis to identify the impact of the factors associated with collision involvement\nConduct multinomial or ordered logistic regression to identify the impact of the factors associated with collision severity; this should have adjustments that enable to do causal analysis, but I don’t know which adjustments yet."
  },
  {
    "objectID": "workposts/driving-data/index.html#wilcoxon-rank-sum",
    "href": "workposts/driving-data/index.html#wilcoxon-rank-sum",
    "title": "Driving analytics for risk assessment in usage-based insurance",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/ericsson-project/index.html",
    "href": "workposts/ericsson-project/index.html",
    "title": "Development of Polar Chart for Model Interpretability",
    "section": "",
    "text": "Here I report the development of a glyph-based polar chart (GPC), designed to support a comprehensive interpretation of the results of ML models by enabling data scientists to compare different explanation methods within the same model and across models.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "I offer short-term consulting, project-based assessments, and ongoing advisory services to optimize human-AI collaboration and improve performance, usability, and trust.\n\n🔹 Human-AI Performance Optimization\nI assess AI-driven systems to enhance usability, trust, and operator performance, reducing cognitive overload in high-risk environments.\n\n\n🔹 Explainability & Decision Support\nI help create AI systems that are transparent, interpretable, and aligned with regulatory requirements, ensuring effective human-AI collaboration.\n\n\n🔹 Training & Workshops\n✔ Human Factors in AI: Designing automation that empowers operators and enhances safety.\n✔ AI Explainability & Compliance: Making AI systems trustworthy, transparent, and regulation-ready.\n✔ Cognitive Load & Performance: Optimizing AI interfaces to support faster, better human decisions.\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "serviceposts/audit/index.html",
    "href": "serviceposts/audit/index.html",
    "title": "Model audit",
    "section": "",
    "text": "This is a time series analysis\n📄 Report\n📊 Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "serviceposts/audit/index.html#wilcoxon-rank-sum",
    "href": "serviceposts/audit/index.html#wilcoxon-rank-sum",
    "title": "Model audit",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/data/index.html",
    "href": "servicesposts/data/index.html",
    "title": "Data strategy",
    "section": "",
    "text": "Develop a data strategy that aligns with your business goals, focusing on data collection, analysis, and experimentation.\n📄 Report\n📊 Slides\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "servicesposts/data/index.html#wilcoxon-rank-sum",
    "href": "servicesposts/data/index.html#wilcoxon-rank-sum",
    "title": "Data strategy",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/audit/index.html",
    "href": "servicesposts/audit/index.html",
    "title": "Model audits",
    "section": "",
    "text": "Assess the performance of your models (predictive, generative), identify biases and ensure robustness to support accurate and reliable outcomes.\n📄 Report\n📊 Slides\n# Libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load dataset from github\ndata &lt;- read.table(\"https://raw.githubusercontent.com/holtzy/data_to_viz/master/Example_dataset/3_TwoNumOrdered.csv\", header=T)\ndata$date &lt;- as.Date(data$date)\n\n# Plot\ndata %&gt;%\n  tail(10) %&gt;%\n  ggplot( aes(x=date, y=value)) +\n    geom_line() +\n    geom_point()"
  },
  {
    "objectID": "servicesposts/audit/index.html#wilcoxon-rank-sum",
    "href": "servicesposts/audit/index.html#wilcoxon-rank-sum",
    "title": "Model audits",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "servicesposts/inference/index.html",
    "href": "servicesposts/inference/index.html",
    "title": "Causal inference",
    "section": "",
    "text": "Uncover the cause-and-effect relationships in your data using methodologies like randomized controlled trials and regression discontinuity.\n\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#menu-sidebar",
    "href": "index.html#menu-sidebar",
    "title": "Frameset Example Title",
    "section": "Menu (Sidebar)",
    "text": "Menu (Sidebar)\nYou can create a sidebar with R Markdown using htmltools or simply present the menu in a separate section. If you prefer to embed the menu HTML directly:"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "Frameset Example Title",
    "section": "Content",
    "text": "Content\nThis is the main content area, which was originally the “content” frame. You can embed the HTML content like so:"
  },
  {
    "objectID": "workposts/hf-nuclear/index.html",
    "href": "workposts/hf-nuclear/index.html",
    "title": "Human performance in operation of automated nuclear reactors",
    "section": "",
    "text": "This research program collects human performance data during simulated operations of Small Modular Reactors (SMRs). SMRs allow for remote monitoring and control of multiple facilities, which introduces a new set of human factors challenges related to the impact of control room design on human performance. Ongoing human-subjects experiments are investigating several aspects of SMR design and their effects on operator behavior, decision-making, and interaction with automated technologies.\n\n\n\nThe basic principle iPWR simulator setup in the Halden Future Lab"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Below are selected publications and presentations on AI usability, human factors, and regulatory considerations in high-stakes industries:\n\nPeer reviewed\n\nGentile D., Donmez, B., & Jamieson, G. A. (2025). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. International Journal of Human-Computer Studies, 196, 103434. Link\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\nIn preparation\n\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transparency on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\nPresentations\n\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI ’21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/publications/index.html",
    "href": "workposts/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "List of academic publications\n\nPeer reviewed journal articles\n\nGoogle Scholar\nGentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\n\nPeer reviewed conference/workshop proceedings\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\n\nArticles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI ’21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/1-hf-nuclear/index.html",
    "href": "workposts/1-hf-nuclear/index.html",
    "title": "Human performance in operation of automated nuclear reactors",
    "section": "",
    "text": "This research program collects human performance data during simulated operations of Small Modular Reactors (SMRs). SMRs allow for remote monitoring and control of multiple facilities, which introduces a new set of human factors challenges related to the impact of control room design on human performance. Ongoing human-subjects experiments are investigating several aspects of SMR design and their effects on operator behavior, decision-making, and interaction with automated technologies.\n\n\n\nThe basic principle iPWR simulator setup in the Halden Future Lab"
  },
  {
    "objectID": "workposts/4-publications/index.html",
    "href": "workposts/4-publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "List of academic publications\n\nPeer reviewed journal articles\n\nGoogle Scholar\nGentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\n\nPeer reviewed conference/workshop proceedings\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link\n\n\n\n\nArticles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI ’21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto."
  },
  {
    "objectID": "workposts/2-explanations-human-performance/index.html",
    "href": "workposts/2-explanations-human-performance/index.html",
    "title": "Human-AI collaboration in industrial process control",
    "section": "",
    "text": "This project focuses on developing information content in the HMI* to support human decision-making in complex environments. The goal is to enhance the interpretability and usability of automated advice, ensuring that the information provided in the HMI aligns with human cognitive processes and operational needs.\nHMI = human machine interface."
  },
  {
    "objectID": "workposts/2-explanations-human-performance/index.html#wilcoxon-rank-sum",
    "href": "workposts/2-explanations-human-performance/index.html#wilcoxon-rank-sum",
    "title": "Designing Machine Feedback that Supports Human Decisions",
    "section": "17.34, Wilcoxon rank sum",
    "text": "17.34, Wilcoxon rank sum\n\n# Prepare the data\ndata2 = data.frame(instr = c(\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\"A\",\n                            \"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\"B\",\n                            \"B\",\"B\",\"B\",\"B\"),\n                   grades = c(88, 75, 92, 71, 63, 84, 55, 64, 82, 96, 72, \n                              65, 84, 53, 76, 80, 51, 60, 57, 85, 94, 87, \n                              73, 61))\ndata2$rank = rank(data2$grades, ties.method = c(\"average\"))\n\n\nR1 = sum(data2$rank[data2$instr == \"A\"])\nR2 = sum(data2$rank[data2$instr == \"B\"])\nN1 = 10\nN2= 14\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1]  53.5000000  70.0000000 291.6666667  -0.9661411\n\n\nFrom the z table, we get a critical x value of -1.96, while our calculated z value is approximately -0.966. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two instructors’ grades.\n\n17.51\n\n\nKruskall Wallis\n\n# Prepare the data\ndata3 = data.frame(method = c(\"1\",\"1\",\"1\",\"1\",\"1\",\n                              \"2\",\"2\",\"2\",\"2\",\"2\",\n                              \"3\",\"3\",\"3\",\"3\",\"3\"),\n                   grades = c(78, 62, 71, 58, 73, \n                              76, 85, 77, 90, 87, \n                              74, 79, 60, 75, 80))\ndata3$rank = rank(data3$grades, ties.method = c(\"average\"))\n\n\n# Calculate H\nR1 = sum(data3$rank[data3$method == \"1\"])\nR2 = sum(data3$rank[data3$method == \"2\"])\nR3 = sum(data3$rank[data3$method == \"3\"])\n\nH = ((12/(15*16)) * ((R1^2)/5 + (R2^2)/5 + (R3^2)/5)) - 3*(16)\nH\n\n[1] 6.54\n\n\nThe critical H value for the .05 significance level is 5.991. Thus, at the .05 significance level, we reject the null hypothesis in favor of the alternative hypothesis that there is difference between the three teaching methods. The critical H value for the .01 significance level is 9.21. Thus, at the .01 significance level, we have not enough evidence to reject the null hypothesis that there is no difference between the three teaching methods.\nIf all five students were the same for each method, the samples would be dependent, where ‘student’ becomes the random factor. Thus, the Friedman test is appropriate in this case.\n\n# Prepare the data\ndata4 = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                   met1 = c(78, 62, 71, 58, 73),\n                   met2 = c(76, 85, 77, 90, 87),\n                   met3 = c(74, 79, 60, 75, 80))\n\ndata4rank = data.frame(student = c(\"1\",\"2\",\"3\",\"4\",\"5\"),\n                       met1 = c(1, 3, 2, 3, 3),\n                       met2 = c(2, 1, 1, 1, 1),\n                       met3 = c(3, 2, 3, 2, 2))\n\n\n# Calculate Chi square\nR1 = sum(data4rank$met1)\nR2 = sum(data4rank$met2)\nR3 = sum(data4rank$met3)\n\nXsq = (12/(5*3*(3+1)))*((R1^2)+(R2^2)+(R3^2)) - 15*4\nXsq\n\n[1] 4.8\n\n\nThe critical Chi square value for the .05 significance level is 5.991. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference. The critical Chi square value for the .01 significance level is 9.21. We fail to reject the null hypothesis .# critH for significance level of .01 is 9.21. Thus, we also fail to reject the null hypothesis at the .01 significance level.\nThe first analysis is testing whether different groups of students undertaking three different teaching methods perform equally in their exam. On the other hand, the second analysis is testing whether the same students perform differently on their exam based on different teaching methods. When different groups of students are subject to different teaching methods, there is difference in grades only at the .05 significance level. When the same students are subject to the three different teaching methods, there is no difference in grades. This might be because in the second analysis ‘student’ is a random factor and thus grades depend more on individual variability rather than teaching method.\n\n\n17.69\n\n# Prepare the data\nfirstj = c(5,2,8,1,4,6,3,7)\nsecondj= c(4,5,7,3,2,8,1,6)\n\n\n# a and b\ncor.test(firstj, secondj, method=c(\"spearman\"))\n\n\n    Spearman's rank correlation rho\n\ndata:  firstj and secondj\nS = 28, p-value = 0.08309\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6666667 \n\n\nThe coefficient of rank correlation is 0.6666667, which is a large effect size. However, according to this test, the correlation is not significant: p-value = 0.08309.\n\n# Alternative: Wilcoxon rank sum test  method to calculate significance \nR1 = sum(firstj)\nR2 = sum(secondj)\nN1 = 8\nN2= 8\n\nU = N1*N2 + ((N1*(N1+1))/2) - R1\nmu = (N1*N2)/2\nvar = (N1*N2*(N1+N2+1))/12\nz = (U-mu)/sqrt(var)\nc(U, mu, var, z)\n\n[1] 64.000000 32.000000 90.666667  3.360672\n\n\nThe critical z value at .05 significance is -1.96, while the calculated z value is approximately 3.36. Thus, accroding to the Wilcoxon rank sum test, the rankings from the two judges differ.\n\n# c: Perform the appropriate non-parametric comparison test\nshapiro.test(firstj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  firstj\nW = 0.97486, p-value = 0.9332\n\nshapiro.test(secondj)\n\n\n    Shapiro-Wilk normality test\n\ndata:  secondj\nW = 0.97486, p-value = 0.9332\n\ncor.test(firstj, secondj, method=c(\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  firstj and secondj\nt = 2.1909, df = 6, p-value = 0.07099\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.07168044  0.93302248\nsample estimates:\n      cor \n0.6666667 \n\n\nThe results from part a and c agree.\n\n\n12.39\nThe null hypothesis is that there is no difference between the sleeping and the sugar pills. The grand total is 170. The expected values are calculated by multiplying the RowSum with the ColSum and dividing that by the grand total: E1 is 39.7, E2 is 14.3, E3 is 85.3 and E4 is 30.7.\n\nXsq = (((44-39.7)^2)/39.7) + (((10-14.3)^2)/14.3) + (((81-85.3)^2)/85.3) + \n  (((35-30.7)^2)/30.7)\nXsq\n\n[1] 2.577795\n\n\n\nDOF = (2-1)*(2-1)\nDOF\n\n[1] 1\n\n\nThe critical value of X square for 1 degree of freedom is 3.84. Thus, at the .05 significance level, we fail to reject the null hypothesis that there is no difference between the two types of pills.\n\nC = sqrt((Xsq^2)/((Xsq^2)+170))\nC\n\n[1] 0.1939535\n\n\nThe coefficient of contingency is 0.1939535. Thus, the variables are independent from each other (i.e., there is no association).\n\n\nSupplementary problem\nThe problem involves a contingency table with dependent samples. The P value answers this question: if there is no association between display type and seeing a collision, what is the probability of observing such a large discrepancy (or larger) between the number of the two kinds of discordant pairs? A small P value is evidence that there is an association between display type and seeing a collision.\nThe two-tailed P value equals 0.1687, which is considered to be not statistically significant. The P value was calculated with McNemar’s test with the continuity correction. Chi squared equals 1.895, with 1 degree of freedom. So we can conclude that there is no difference between displays."
  },
  {
    "objectID": "workposts/3-ericsson-project/index.html",
    "href": "workposts/3-ericsson-project/index.html",
    "title": "UX analysis of explainable machine learning tools for Ericsson’s data scientists",
    "section": "",
    "text": "During my internship at Ericsson, we developed and evaluated a glyph-based polar chart (GPC) designed to support a comprehensive interpretation of the results of ML models. The GPC enabled data scientists to compare different explanation methods within the same model and across models."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Davide Gentile",
    "section": "",
    "text": "Human Factors in SMR Operations\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning Machine Feedback to Assist Human Decisions\n\n\n\n\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment of Polar Chart for Model Interpretability\n\n\n\n\n\n\nAug 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Portfolio"
    ]
  },
  {
    "objectID": "blogposts/conducting a socio-technical evaluation/index.html",
    "href": "blogposts/conducting a socio-technical evaluation/index.html",
    "title": "Evaluating joint human-automation performance",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "publications.html#peer-reviewed-journal-articles",
    "href": "publications.html#peer-reviewed-journal-articles",
    "title": "Davide Gentile",
    "section": "",
    "text": "Gentile D., Donmez, B., & Jamieson, G. A. (Accepted, 2024). Human performance effects of combining counterfactual explanations with normative and contrastive explanations in supervised machine learning for automated decision assistance. In the International Journal of Human-Computer Studies.\nGentile D., Donmez, B., & Jamieson, G. A. (2023). Human Performance Consequences of Normative and Contrastive Explanations: An Experiment in Machine Learning for Reliability Maintenance. Artificial Intelligence, 103945. Link\nNguyen, T., Gentile D., Jamieson, G. A., Gosine, R., & Purmhedi, H. (2023). Designing a Glyph-based Polar Chart to Interpret the Results of Machine Learning Models. In Ergonomics in Design: The Quarterly of Human Factors Applications. Link\n\n\n\n\n\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating Human Understanding in Explainable AI Systems. In ACM Human Factors in Computing Systems (CHI) Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI2021, Yokohama, Japan. Link",
    "crumbs": [
      "Publications"
    ]
  },
  {
    "objectID": "publications.html#articles-in-preparation",
    "href": "publications.html#articles-in-preparation",
    "title": "Davide Gentile",
    "section": "Articles in preparation",
    "text": "Articles in preparation\nGentile D., & Jamieson, G. A. (2025). Effects of Automation transpareancy on human performance under different workload contrainsts. Human Factors.\nLawson-Jack K., Gentile D., & Jamieson, G. A. (2025). Progress on a Taxonomy of Heterogeneity in Small Modular Reactor Operations. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Liang Y., & Jamieson, G. A. (2025). Assessing Measures of Human Performance in the Nuclear Control Room. In the 14th International Topical Meeting on Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT), Chicago.\nGentile D., Jamieson, G. A., & Donmez B. (2023). Influence of Individual Differences on the Effectiveness of Post-hoc Explanations in Automation Reliance Behavior. Submitted to the Journal of Cognitive Engineering and Decision-Making.\n\n\n\nPresentations\nGentile D., Jamieson G. A., Donmez B. (2024). Supporting human performance with explanation interfaces in automated decision assistance for process control operations. In Disruptive, Innovative and Emerging Technologies (DIET) Conference 2024 of the Canadian Nuclear Society (CNS) (organized in cooperation with the International Atomic Energy Agency).\nGentile D., Donmez, B., & Jamieson, G. A. (2024). Enhancing Human Performance with Post-hoc Explanations in Machine Learning-based Decision Support Systems. Oral presentation at the 7th International Conference on Intelligent Human Systems Integration: Integrating People and Intelligent Systems, Palermo, Italy.\nGentile D. (2023). Designing for human-AI interactions in industrial condition monitoring. Oral presentation at the AI Seminar in GAIA (Global Artificial Intelligence Accelerator), Ericsson Montreal, QC.\nGentile D., Jamieson G. A., Donmez B. (2021). Evaluating human understanding in XAI systems. Oral presentation at the ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI, CHI Conference on Human Factors in Computing Systems (CHI ’21), Yokohama, Japan (held remotely).\nGentile D. (2020). Human Factors in Explainable AI. Invited panelist at the 2021 Graduate Student Research Showcase. Faculty of Applied Science and Engineering, University of Toronto.",
    "crumbs": [
      "Publications"
    ]
  },
  {
    "objectID": "blogposts/user-interaction-llms/index.html",
    "href": "blogposts/user-interaction-llms/index.html",
    "title": "Exploring user interaction challenges with large language models",
    "section": "",
    "text": "Available here."
  },
  {
    "objectID": "about.html#hello",
    "href": "about.html#hello",
    "title": "Davide Gentile",
    "section": "",
    "text": "I am a data science and R&D leader with a background in cognitive science of language and human factors engineering. My experience spans academic research, industrial R&D, and applied data science.\nI have a strong foundation in behavioral science, machine learning, and quantitative UX research, and specialize in developing data solutions that enhance human decision-making and optimize user experiences. I thrive in cross-functional environments, collaborating with engineers, designers, and business leaders to align advanced analytics with real-world applications. My goal is to make data work for people, not the other way around."
  },
  {
    "objectID": "workposts/4-penguins/index.html",
    "href": "workposts/4-penguins/index.html",
    "title": "Penguin",
    "section": "",
    "text": "This analysis takes the best from three analysis on the Penguin data.\nShowing mostly a PCA, and Simpson’s paradox.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"penguins\")\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nhttps://allisonhorst.github.io/palmerpenguins/\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3701. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.5          15.0              217.       5076. 2008.\n\n\nFor this example:\n\nshorten variable names (remove units) to simplify variable labels,\ncreate factors for character variables (needed for MANOVA), and\nremove NA observations (causes problems with PCA)\n\n\npeng &lt;- penguins %&gt;%\n    dplyr::rename(\n         bill_length = bill_length_mm, \n         bill_depth = bill_depth_mm, \n         flipper_length = flipper_length_mm, \n         body_mass = body_mass_g)\npeng &lt;- peng %&gt;% drop_na()\npeng\n\n# A tibble: 333 × 8\n   species island    bill_length bill_depth flipper_length body_mass sex    year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;     &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen        39.1       18.7            181      3750 male   2007\n 2 Adelie  Torgersen        39.5       17.4            186      3800 fema…  2007\n 3 Adelie  Torgersen        40.3       18              195      3250 fema…  2007\n 4 Adelie  Torgersen        36.7       19.3            193      3450 fema…  2007\n 5 Adelie  Torgersen        39.3       20.6            190      3650 male   2007\n 6 Adelie  Torgersen        38.9       17.8            181      3625 fema…  2007\n 7 Adelie  Torgersen        39.2       19.6            195      4675 male   2007\n 8 Adelie  Torgersen        41.1       17.6            182      3200 fema…  2007\n 9 Adelie  Torgersen        38.6       21.2            191      3800 male   2007\n10 Adelie  Torgersen        34.6       21.1            198      4400 male   2007\n# ℹ 323 more rows\n\n\n\nlibrary(car)\nlibrary(ggbiplot)\nlibrary(GGally)\n\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n                  data=peng,\n                  ellipse=list(levels=0.68),\n                  col = scales::hue_pal()(3),\n                  legend=list(coords=\"bottomright\"))\n\n\n\n\n\n\n\n\n\nggpairs(peng, mapping = aes(color = species), \n        columns = c(\"bill_length\", \"bill_depth\", \n                    \"flipper_length\", \"body_mass\",\n                    \"island\", \"sex\"))\n\n\n\n\n\n\n\n\n\npeng.pca &lt;- prcomp (~ bill_length + bill_depth + flipper_length + body_mass,\n                    data=peng,\n                    scale. = TRUE)\n\npeng.pca\n\nStandard deviations (1, .., p=4):\n[1] 1.6569115 0.8821095 0.6071594 0.3284579\n\nRotation (n x k) = (4 x 4):\n                      PC1         PC2        PC3        PC4\nbill_length     0.4537532 -0.60019490 -0.6424951  0.1451695\nbill_depth     -0.3990472 -0.79616951  0.4258004 -0.1599044\nflipper_length  0.5768250 -0.00578817  0.2360952 -0.7819837\nbody_mass       0.5496747 -0.07646366  0.5917374  0.5846861\n\n\n\nscreeplot(peng.pca, type = \"line\", lwd=3, cex=3, \n        main=\"Variances of PCA Components\")\n\n\n\n\n\n\n\n\n\nggbiplot(peng.pca, obs.scale = 1, var.scale = 1,\n         groups = peng$species, \n         ellipse = TRUE, circle = TRUE) +\n  scale_color_discrete(name = 'Penguin Species') +\n  theme_minimal() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\n\n\n\n\n\n\nFrom this, we can see:\n\nThese two principal components account for 68.6 + 19.5 = 88.1 % of the total variance of these four size variables.\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size.\nOn this dimension, Gentoos are the largest, by quite a lot, compared with Adelie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape."
  },
  {
    "objectID": "workposts/4-ubi/index.html",
    "href": "workposts/4-ubi/index.html",
    "title": "Evaluation of machine learning models for usage-based insurance",
    "section": "",
    "text": "This analysis takes the best from three analysis on the Penguin data.\nShowing mostly a PCA, and Simpson’s paradox.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"penguins\")\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nhttps://allisonhorst.github.io/palmerpenguins/\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3701. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.5          15.0              217.       5076. 2008.\n\n\nFor this example:\n\nshorten variable names (remove units) to simplify variable labels,\ncreate factors for character variables (needed for MANOVA), and\nremove NA observations (causes problems with PCA)\n\n\npeng &lt;- penguins %&gt;%\n    dplyr::rename(\n         bill_length = bill_length_mm, \n         bill_depth = bill_depth_mm, \n         flipper_length = flipper_length_mm, \n         body_mass = body_mass_g)\npeng &lt;- peng %&gt;% drop_na()\npeng\n\n# A tibble: 333 × 8\n   species island    bill_length bill_depth flipper_length body_mass sex    year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;     &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen        39.1       18.7            181      3750 male   2007\n 2 Adelie  Torgersen        39.5       17.4            186      3800 fema…  2007\n 3 Adelie  Torgersen        40.3       18              195      3250 fema…  2007\n 4 Adelie  Torgersen        36.7       19.3            193      3450 fema…  2007\n 5 Adelie  Torgersen        39.3       20.6            190      3650 male   2007\n 6 Adelie  Torgersen        38.9       17.8            181      3625 fema…  2007\n 7 Adelie  Torgersen        39.2       19.6            195      4675 male   2007\n 8 Adelie  Torgersen        41.1       17.6            182      3200 fema…  2007\n 9 Adelie  Torgersen        38.6       21.2            191      3800 male   2007\n10 Adelie  Torgersen        34.6       21.1            198      4400 male   2007\n# ℹ 323 more rows\n\n\n\nlibrary(car)\nlibrary(ggbiplot)\nlibrary(GGally)\n\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n                  data=peng,\n                  ellipse=list(levels=0.68),\n                  col = scales::hue_pal()(3),\n                  legend=list(coords=\"bottomright\"))\n\n\n\n\n\n\n\n\n\nggpairs(peng, mapping = aes(color = species), \n        columns = c(\"bill_length\", \"bill_depth\", \n                    \"flipper_length\", \"body_mass\",\n                    \"island\", \"sex\"))\n\n\n\n\n\n\n\n\n\npeng.pca &lt;- prcomp (~ bill_length + bill_depth + flipper_length + body_mass,\n                    data=peng,\n                    scale. = TRUE)\n\npeng.pca\n\nStandard deviations (1, .., p=4):\n[1] 1.6569115 0.8821095 0.6071594 0.3284579\n\nRotation (n x k) = (4 x 4):\n                      PC1         PC2        PC3        PC4\nbill_length     0.4537532 -0.60019490 -0.6424951  0.1451695\nbill_depth     -0.3990472 -0.79616951  0.4258004 -0.1599044\nflipper_length  0.5768250 -0.00578817  0.2360952 -0.7819837\nbody_mass       0.5496747 -0.07646366  0.5917374  0.5846861\n\n\n\nscreeplot(peng.pca, type = \"line\", lwd=3, cex=3, \n        main=\"Variances of PCA Components\")\n\n\n\n\n\n\n\n\n\nggbiplot(peng.pca, obs.scale = 1, var.scale = 1,\n         groups = peng$species, \n         ellipse = TRUE, circle = TRUE) +\n  scale_color_discrete(name = 'Penguin Species') +\n  theme_minimal() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\n\n\n\n\n\n\nFrom this, we can see:\n\nThese two principal components account for 68.6 + 19.5 = 88.1 % of the total variance of these four size variables.\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size.\nOn this dimension, Gentoos are the largest, by quite a lot, compared with Adelie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape.\n\n\n# Scatterplot example 2: penguin bill length versus bill depth\nggplot(data = peng, aes(x = bill_length, y = bill_depth)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 2)  +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) +\n  theme_minimal()"
  },
  {
    "objectID": "courses2.html",
    "href": "courses2.html",
    "title": "Courses2",
    "section": "",
    "text": "Introduction to data science with R\nTime series analysis with R\nStatistical models for empirical research\nDesign of experiments\nData visualization and communication\nProgramming practices in human-subjects research\nHow to publish scientific research"
  },
  {
    "objectID": "workposts/ubi/index.html",
    "href": "workposts/ubi/index.html",
    "title": "Evaluation of machine learning models for usage-based insurance",
    "section": "",
    "text": "This analysis takes the best from three analysis on the Penguin data.\nShowing mostly a PCA, and Simpson’s paradox.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"penguins\")\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nhttps://allisonhorst.github.io/palmerpenguins/\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3701. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.5          15.0              217.       5076. 2008.\n\n\nFor this example:\n\nshorten variable names (remove units) to simplify variable labels,\ncreate factors for character variables (needed for MANOVA), and\nremove NA observations (causes problems with PCA)\n\n\npeng &lt;- penguins %&gt;%\n    dplyr::rename(\n         bill_length = bill_length_mm, \n         bill_depth = bill_depth_mm, \n         flipper_length = flipper_length_mm, \n         body_mass = body_mass_g)\npeng &lt;- peng %&gt;% drop_na()\npeng\n\n# A tibble: 333 × 8\n   species island    bill_length bill_depth flipper_length body_mass sex    year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;     &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen        39.1       18.7            181      3750 male   2007\n 2 Adelie  Torgersen        39.5       17.4            186      3800 fema…  2007\n 3 Adelie  Torgersen        40.3       18              195      3250 fema…  2007\n 4 Adelie  Torgersen        36.7       19.3            193      3450 fema…  2007\n 5 Adelie  Torgersen        39.3       20.6            190      3650 male   2007\n 6 Adelie  Torgersen        38.9       17.8            181      3625 fema…  2007\n 7 Adelie  Torgersen        39.2       19.6            195      4675 male   2007\n 8 Adelie  Torgersen        41.1       17.6            182      3200 fema…  2007\n 9 Adelie  Torgersen        38.6       21.2            191      3800 male   2007\n10 Adelie  Torgersen        34.6       21.1            198      4400 male   2007\n# ℹ 323 more rows\n\n\n\nlibrary(car)\nlibrary(ggbiplot)\nlibrary(GGally)\n\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n                  data=peng,\n                  ellipse=list(levels=0.68),\n                  col = scales::hue_pal()(3),\n                  legend=list(coords=\"bottomright\"))\n\n\n\n\n\n\n\n\n\nggpairs(peng, mapping = aes(color = species), \n        columns = c(\"bill_length\", \"bill_depth\", \n                    \"flipper_length\", \"body_mass\",\n                    \"island\", \"sex\"))\n\n\n\n\n\n\n\n\n\npeng.pca &lt;- prcomp (~ bill_length + bill_depth + flipper_length + body_mass,\n                    data=peng,\n                    scale. = TRUE)\n\npeng.pca\n\nStandard deviations (1, .., p=4):\n[1] 1.6569115 0.8821095 0.6071594 0.3284579\n\nRotation (n x k) = (4 x 4):\n                      PC1         PC2        PC3        PC4\nbill_length     0.4537532 -0.60019490 -0.6424951  0.1451695\nbill_depth     -0.3990472 -0.79616951  0.4258004 -0.1599044\nflipper_length  0.5768250 -0.00578817  0.2360952 -0.7819837\nbody_mass       0.5496747 -0.07646366  0.5917374  0.5846861\n\n\n\nscreeplot(peng.pca, type = \"line\", lwd=3, cex=3, \n        main=\"Variances of PCA Components\")\n\n\n\n\n\n\n\n\n\nggbiplot(peng.pca, obs.scale = 1, var.scale = 1,\n         groups = peng$species, \n         ellipse = TRUE, circle = TRUE) +\n  scale_color_discrete(name = 'Penguin Species') +\n  theme_minimal() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\n\n\n\n\n\n\nFrom this, we can see:\n\nThese two principal components account for 68.6 + 19.5 = 88.1 % of the total variance of these four size variables.\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size.\nOn this dimension, Gentoos are the largest, by quite a lot, compared with Adelie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape.\n\n\n# Scatterplot example 2: penguin bill length versus bill depth\nggplot(data = peng, aes(x = bill_length, y = bill_depth)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 2)  +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) +\n  theme_minimal()"
  },
  {
    "objectID": "workposts/human-ai-collab/index.html",
    "href": "workposts/human-ai-collab/index.html",
    "title": "Human-AI collaboration in industrial process control",
    "section": "",
    "text": "This project focuses on developing information content in the HMI* to support human decision-making in complex environments. The goal is to enhance the interpretability and usability of automated advice, ensuring that the information provided in the HMI aligns with human cognitive processes and operational needs.\nHMI = human machine interface."
  },
  {
    "objectID": "workposts/ux-ericsson/index.html",
    "href": "workposts/ux-ericsson/index.html",
    "title": "UX analysis of explainable machine learning tools for Ericsson’s data scientists",
    "section": "",
    "text": "During my internship at Ericsson, we developed and evaluated a glyph-based polar chart (GPC) designed to support a comprehensive interpretation of the results of ML models. The GPC enabled data scientists to compare different explanation methods within the same model and across models."
  },
  {
    "objectID": "portfolio/ux-ericsson/index.html",
    "href": "portfolio/ux-ericsson/index.html",
    "title": "Machine Learning Model Interpretability Tool",
    "section": "",
    "text": "This project developed a glyph-based polar chart (GPC) to enhance the interpretability of machine learning models for data scientists. The tool enables comparisons of explanations across different models and computational methods. User experience evaluations with Ericsson data scientists showed that the GPC helped identify key model variables, compare various explanation techniques, and perform logical reviews of model outputs. This project was conducted during my internship at Ericsson’s Global AI Accelerator.\nDuring my internship at Ericsson, we developed and evaluated a glyph-based polar chart (GPC) designed to support a comprehensive interpretation of the results of ML models. The GPC enabled data scientists to compare different explanation methods within the same model and across models."
  },
  {
    "objectID": "portfolio/human-ai-collab/index.html",
    "href": "portfolio/human-ai-collab/index.html",
    "title": "Model-Agnostic Explanations for Industrial AI",
    "section": "",
    "text": "This project investigated the impact of model-agnostic explanations on human performance in industrial process control, focusing on improving the detection of system failures in condition-based maintenance. Two controlled experiments tested the effects of different explanation types—normative, contrastive, and counterfactual—on decision-making, workload, and reliance on automated decision aids. Results showed that combining normative and contrastive explanations improved decision time and reduced workload, while adding counterfactuals further enhanced accuracy and reduced false alarms. The findings can inform the design of more effective and efficient explainable AI systems to support human operators in safety-critical work environments.\nThis project focuses on developing information content in the HMI* to support human decision-making in complex environments. The goal is to enhance the interpretability and usability of automated advice, ensuring that the information provided in the HMI aligns with human cognitive processes and operational needs.\nHMI = human machine interface."
  },
  {
    "objectID": "portfolio/hf-nuclear/index.html",
    "href": "portfolio/hf-nuclear/index.html",
    "title": "Human Factors in Small Modular Reactor Operations",
    "section": "",
    "text": "Context\nThe increasing automation of small modular reactors (SMRs) introduces new challenges for human operators in nuclear control rooms. Effective monitoring and control require maintaining situation awareness and workload balance, but traditional assessment methods lack validity and reliability in these high-stakes environments. Additionally, the diversity of SMR designs complicates the development of standardized human factors guidelines to ensure safe and efficient operation.\nStarting September 2024, I am leading experimental and analytical projects in a new research program aimed at addressing these gaps. The research program focuses on two key objectives: improving methodologies for assessing situation awareness (SA) and workload in nuclear control rooms, and developing a taxonomy to categorize SMR designs and their impact on human performance. The first experiment tests multiple (SA) and workload measures across different automation levels. A parallel project is creating a taxonomy to guide human factors guidelines for varying SMR designs.\nThe project aims to establish better performance metrics and standardized operational guidelines, supporting safety and efficiency in the nuclear sector. This research is funded by the Natural Sciences and Engineering Council of Canada and the Canadian Nuclear Safety Commission.\n\n\nMethods\nThis research program addresses these challenges through two parallel projects:\n\nProject 1: Experiment on Operator Performance & Automation\n\nI designed an experiment using the RANCOR microworld platform to simulate the operation of small modular reactors under routine and abnormal scenarios.\nTested 24 participants and collected multiple metrics of situation awareness and workload across three levels of automation in computerized procedures. The three levels of automation in the computerized procedures are defined according to the Institute of Electrical and Electronics Engineers (IEEE) Standard 1786 (IEEE, 2022): Type I procedure displays the instructions on a screen, resembling a traditional paper-based procedure; Type II can additionally display process data and step logic, visualize results, and provide access links to displays and soft controls; Type III has the additional capability to automatically carry out sequences in the procedure and has embedded soft control features.\nEvaluated situation awareness, cognitive workload, decision-making, and reaction time using a within-subjects experimental design with three counterbalanced conditions corresponding to the level of automation in the computerized procedures (Table I).\n\n\n\n\nTable I. Experimental design. A = Type I procedure; B = Type II; C = Type III.\n\n\n\n\nProject 2: Development of an SMR Taxonomy\n\nMy team is leading the development of a taxonomy that categorizes key design variations in SMRs, including levels of automation, control systems, and operational phases. This work identifies implications for task performance, cognitive workload, and situational awareness, providing a structured framework to inform human factors guidelines and improve safety in nuclear operations.\n\n\n\n\nKey Findings\nFindings will be presented at the Nuclear Plant Instrumentation and Control & Human-Machine Interface Technology (NPIC&HMIT) meeting in Chicago in June 2025.\n\n\nImplications\nThis research supports regulatory agencies and nuclear operators in defining best practices for safe SMR operations by providing validated methodologies for assessing operator performance. It also helps engineers and system designers optimize automation levels and control interfaces to enhance human performance in nuclear control rooms.\n\n\n\nSMR simulator setup in the Halden Future Lab, Norway.\n\n\n\nFurther Reading\n\nIEEE. (2022). IEEE Guide for Human Factors Applications of Computerized Operating Procedure Systems (COPS) at Nuclear Power Generating Stations and Other Nuclear Facilities, IEEE- 1786-2022. New York: IEEE."
  },
  {
    "objectID": "portfolio/ubi/index.html",
    "href": "portfolio/ubi/index.html",
    "title": "Evaluation of machine learning models for usage-based insurance",
    "section": "",
    "text": "This analysis takes the best from three analysis on the Penguin data.\nShowing mostly a PCA, and Simpson’s paradox.\n\nlibrary(palmerpenguins)\nlibrary(dplyr)\nlibrary(tidyr)\ndata(\"penguins\")\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nhttps://allisonhorst.github.io/palmerpenguins/\n\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(across(where(is.numeric), mean, na.rm = TRUE))\n\n# A tibble: 3 × 6\n  species   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  year\n  &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Adelie              38.8          18.3              190.       3701. 2008.\n2 Chinstrap           48.8          18.4              196.       3733. 2008.\n3 Gentoo              47.5          15.0              217.       5076. 2008.\n\n\nFor this example:\n\nshorten variable names (remove units) to simplify variable labels,\ncreate factors for character variables (needed for MANOVA), and\nremove NA observations (causes problems with PCA)\n\n\npeng &lt;- penguins %&gt;%\n    dplyr::rename(\n         bill_length = bill_length_mm, \n         bill_depth = bill_depth_mm, \n         flipper_length = flipper_length_mm, \n         body_mass = body_mass_g)\npeng &lt;- peng %&gt;% drop_na()\npeng\n\n# A tibble: 333 × 8\n   species island    bill_length bill_depth flipper_length body_mass sex    year\n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt;          &lt;int&gt;     &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n 1 Adelie  Torgersen        39.1       18.7            181      3750 male   2007\n 2 Adelie  Torgersen        39.5       17.4            186      3800 fema…  2007\n 3 Adelie  Torgersen        40.3       18              195      3250 fema…  2007\n 4 Adelie  Torgersen        36.7       19.3            193      3450 fema…  2007\n 5 Adelie  Torgersen        39.3       20.6            190      3650 male   2007\n 6 Adelie  Torgersen        38.9       17.8            181      3625 fema…  2007\n 7 Adelie  Torgersen        39.2       19.6            195      4675 male   2007\n 8 Adelie  Torgersen        41.1       17.6            182      3200 fema…  2007\n 9 Adelie  Torgersen        38.6       21.2            191      3800 male   2007\n10 Adelie  Torgersen        34.6       21.1            198      4400 male   2007\n# ℹ 323 more rows\n\n\n\nlibrary(car)\nlibrary(ggbiplot)\nlibrary(GGally)\n\n\nscatterplotMatrix(~ bill_length + bill_depth + flipper_length + body_mass | species,\n                  data=peng,\n                  ellipse=list(levels=0.68),\n                  col = scales::hue_pal()(3),\n                  legend=list(coords=\"bottomright\"))\n\n\n\n\n\n\n\n\n\nggpairs(peng, mapping = aes(color = species), \n        columns = c(\"bill_length\", \"bill_depth\", \n                    \"flipper_length\", \"body_mass\",\n                    \"island\", \"sex\"))\n\n\n\n\n\n\n\n\n\npeng.pca &lt;- prcomp (~ bill_length + bill_depth + flipper_length + body_mass,\n                    data=peng,\n                    scale. = TRUE)\n\npeng.pca\n\nStandard deviations (1, .., p=4):\n[1] 1.6569115 0.8821095 0.6071594 0.3284579\n\nRotation (n x k) = (4 x 4):\n                      PC1         PC2        PC3        PC4\nbill_length     0.4537532 -0.60019490 -0.6424951  0.1451695\nbill_depth     -0.3990472 -0.79616951  0.4258004 -0.1599044\nflipper_length  0.5768250 -0.00578817  0.2360952 -0.7819837\nbody_mass       0.5496747 -0.07646366  0.5917374  0.5846861\n\n\n\nscreeplot(peng.pca, type = \"line\", lwd=3, cex=3, \n        main=\"Variances of PCA Components\")\n\n\n\n\n\n\n\n\n\nggbiplot(peng.pca, obs.scale = 1, var.scale = 1,\n         groups = peng$species, \n         ellipse = TRUE, circle = TRUE) +\n  scale_color_discrete(name = 'Penguin Species') +\n  theme_minimal() +\n  theme(legend.direction = 'horizontal', legend.position = 'top') \n\n\n\n\n\n\n\n\nFrom this, we can see:\n\nThese two principal components account for 68.6 + 19.5 = 88.1 % of the total variance of these four size variables.\nPC1 is largely determined by flipper length and body mass. We can interpret this as an overall measure of penguin size.\nOn this dimension, Gentoos are the largest, by quite a lot, compared with Adelie and Chinstrap.\nPC2 is mainly determined by variation in the two beak variables: bill length and depth. Chinstrap are lower than the other two species on bill length and depth, but bill length further distinguishes the Gentoos. A penguin biologist could almost certainly provide an explanation, but I’ll call this beak shape.\n\n\n# Scatterplot example 2: penguin bill length versus bill depth\nggplot(data = peng, aes(x = bill_length, y = bill_depth)) +\n  geom_point(aes(color = species, \n                 shape = species),\n             size = 2)  +\n  scale_color_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) +\n  theme_minimal()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Portfolio",
    "section": "",
    "text": "Human Factors in Small Modular Reactor Operations\n\n\n\n\n\nThis project focused on improving safety and efficiency in nuclear control rooms by assessing situation awareness and workload in SMR operations. The project includes developing a taxonomy to standardize operational guidelines and performance metrics, supporting the design of more effective human factors practices for emerging nuclear technologies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel-Agnostic Explanations for Industrial AI\n\n\n\n\n\nThis project explored how model-agnostic explanations impact decision-making in industrial process control, particularly for condition-based maintenance. Experiments revealed that combining normative and contrastive explanations enhanced decision speed and accuracy, while counterfactuals reduced false alarms, contributing to safer and more efficient AI systems in industrial settings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Evaluation for Usage-based Insurance\n\n\n\n\n\nThis project presents an analysis of driving contexts and behaviors to improve usage-based insurance models for corporate fleet drivers. We compared machine learning algorithms to predict collision risk in real-time driving behavior data from 3,854 corporate vehicle drivers. The analysis identified key factors that influenced collision involvement, such as driving time, trip frequency, and rapid speed changes, providing fleet rental companies with data-driven insights to adjust insurance rates based on risky driving behaviors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Model Interpretability Tool\n\n\n\n\n\nDuring my internship at Ericsson’s Global AI Accelerator, my team developed a glyph-based polar chart (GPC) to enhance the interpretability of machine learning models, enabling easier comparison of model explanations and identification of key variables. Tested with Ericsson’s data scientists, the tool helped streamline logical reviews of model outputs.\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]